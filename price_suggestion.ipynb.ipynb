{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6bd257e3-f8fb-4bd8-b2cb-4698782b1647","_uuid":"fd3e6dc4a9f244a30354e4fd9b419b744e0e300f","collapsed":true,"trusted":false},"outputs":[],"source":["SUBMISSION = True\n","print(\"SUBMISSION: {}\".format(SUBMISSION))"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"07ba1838-f2e6-4ed8-8a57-8602eb1722a6","_uuid":"c8c3a41aeee3d32de6504ac888c83a43e9161115","collapsed":true,"trusted":false},"outputs":[],"source":["import os \n","import multiprocessing as mp\n","from joblib import Parallel, delayed\n","\n","os.environ['MKL_NUM_THREADS'] = '4'\n","os.environ['OMP_NUM_THREADS'] = '4'\n","os.environ['JOBLIB_START_METHOD'] = 'forkserver'"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2d8c1b2e-0609-4c51-8dd9-46fb3d12bb8e","_uuid":"b292a146380432f3cbe55bd496a13100093d57d7","collapsed":true,"trusted":false},"outputs":[],"source":["import gc\n","import re\n","import math\n","from time import time\n","from collections import Counter\n","import tensorflow as tf\n","import pandas as pd\n","import numpy as np\n","import csv\n","from fastcache import clru_cache as lru_cache\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.corpus import stopwords"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"604a446b-0e5f-4c04-ab9f-339e469949d0","_uuid":"b98afd7324e86515381d7044370332e0d8495ede","collapsed":true,"trusted":false},"outputs":[],"source":["t_start = time()\n","\n","def print_elapsed(text=''):\n","    took = (time() - t_start) / 60.0\n","    print('==== \"%s\" elapsed %.3f minutes' % (text, took))"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ff879f64-83ea-451e-937c-1a2f5c10db65","_uuid":"cb982f7ef742adffcfd2f1ce2c2dbae28110b490","collapsed":true,"trusted":false},"outputs":[],"source":["#Competition metric\n","def rmsle(y, y0):\n","    assert len(y) == len(y0)\n","    return np.sqrt(np.mean(np.power(np.log1p(y) - np.log1p(y0), 2)))"]},{"cell_type":"markdown","metadata":{"_cell_guid":"eba76b26-8eb7-40a5-b3ed-919c23deccea","_uuid":"6d9a5a3395659d6d03e9d4122478838ed8d4e3bd"},"source":["### Loading GloVE"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"667fea7f-3942-472b-9cde-b2fbec7b7acc","_uuid":"76051d5b0a1ca2c887c229f498a81f8789281b0f","collapsed":true,"trusted":false},"outputs":[],"source":["print('Loading GloVE...')\n","\n","GLOVE_PATH = '../input/glove-global-vectors-for-word-representation/glove.6B.50d.txt'\n","embeddings_df = pd.read_table(GLOVE_PATH, sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n","\n","word_embeddings_matrix = embeddings_df.values.astype(np.float32)\n","print('GloVE Word embeddings shape:', word_embeddings_matrix.shape)\n","word_embedding_vocab = {t: i for (i, t) in enumerate(embeddings_df.index.tolist())}\n","\n","del(embeddings_df)\n","\n","print_elapsed()"]},{"cell_type":"markdown","metadata":{"_cell_guid":"55f74c2f-4631-4d25-ac9a-9d145f35edfc","_uuid":"2a03732074ba95ec64d5a8bc31a229c72d8d4d28"},"source":["### Loading data"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"16207dac-f763-4827-bdf6-c77e091c29c6","_uuid":"a4dacfa3ff9727260e235549d6397cf2c2c03a2f","collapsed":true,"trusted":false},"outputs":[],"source":["print('Reading train data...')\n","df_train = pd.read_table('../input/mercari-price-suggestion-challenge/train.tsv', engine='c')\n","print('Train set size: {}'.format(len(df_train)))\n","print_elapsed()"]},{"cell_type":"markdown","metadata":{"_cell_guid":"da49a26d-2abb-4009-ae01-20aa9b0ed350","_uuid":"6c5b8ca8381dc952a1bbd996859407a15c0f8ae4"},"source":["### Feature Engineering"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"03042807-b37e-4c66-8ffb-58ce0df6bd19","_uuid":"71cb31922da46f9067f9955e8dd6c0a1650be1c9","collapsed":true,"trusted":false},"outputs":[],"source":["print('Generating features with statistics for item description textual content')\n","\n","acronyms_regex = re.compile('([A-Z\\-0-9]{2,})')\n","hashtag_regex = re.compile(r'(#[a-z]{2,})')\n","\n","#Extracts statistics for each description, words lengths, like percentage of upper-case words, hashtags, etc\n","def extract_counts(text):\n","    text_size_words_counts = len(text.split(' '))\n","    text_size_words_log_counts = math.log1p(text_size_words_counts)\n","    full_uppercase_perc = len(acronyms_regex.findall(text)) / float(text_size_words_counts)\n","    exclamation_log_count = math.log1p(text.count('!'))\n","    star_log_count = math.log1p(text.count('*'))\n","    percentage_log_count = math.log1p(text.count('%'))\n","    price_removed_marker_log_count = math.log1p(text.count('[rm]'))\n","    hashtag_log_count = math.log1p(len(hashtag_regex.findall(text)))    \n","    return [text_size_words_log_counts,\n","            full_uppercase_perc,\n","            exclamation_log_count,\n","            star_log_count,            \n","            percentage_log_count,\n","            price_removed_marker_log_count,\n","            hashtag_log_count]\n","\n","item_descr_counts = np.vstack(df_train['item_description'].astype(str).apply(extract_counts).values)\n","\n","item_descr_counts_scaler = StandardScaler(copy=True)\n","X_item_descr_counts = item_descr_counts_scaler.fit_transform(item_descr_counts)\n","\n","del(item_descr_counts)\n","\n","print_elapsed()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"82019c37-1e9d-47a8-b93c-1464ee723ec8","_uuid":"e4b509d1c72b41f6d1dee8b677f6be1e2a0de101","collapsed":true,"trusted":false},"outputs":[],"source":["#Removing target attribute (price) from training set, to avoid data leak\n","price = df_train.pop('price')"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"edb00036-1351-4a20-83bf-5cfc9ecbd0d6","_uuid":"162a4183f5ea647275657b6a68fbe8f53a68d2b7","collapsed":true,"trusted":false},"outputs":[],"source":["print('Spliting train/validation set')\n","#Defining train / eval sets\n","valid_rate = 0.00001 if SUBMISSION else 0.1\n","valid_size = int(len(df_train)*valid_rate)\n","\n","np.random.seed(100)\n","rows_idxs = np.arange(0,len(df_train))\n","np.random.shuffle(rows_idxs)\n","\n","valid_idxs = rows_idxs[-valid_size:]\n","\n","#Ignoring lower prices in the train set (minimum price is 3.0 on Mercari website)\n","train_zeroed_prices_idxs = price.iloc[np.in1d(price.index.values, valid_idxs, invert=True)][price < 3.0].index.values\n","train_idxs = price.iloc[np.in1d(price.index.values, np.hstack([valid_idxs, train_zeroed_prices_idxs]), \n","                                invert=True)].index.values\n","\n","#Validating the train / validation set split\n","assert len(df_train) == len(train_zeroed_prices_idxs) + len(train_idxs) + len(valid_idxs)\n","assert len(set(train_idxs).intersection(set(valid_idxs))) == 0\n","\n","train_size = len(train_idxs)\n","\n","del(rows_idxs)\n","print_elapsed()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"88f1279a-e755-4e03-be9c-bbb39f3ac133","_uuid":"a8f7ee33f4dd950d8a06f8afe2611e523b3aa83b","collapsed":true,"trusted":false},"outputs":[],"source":["print('Normalizing price')\n","price_log = np.log1p(price)\n","\n","price_log_train = price_log.iloc[train_idxs].values\n","price_log_train_mean = price_log_train.mean()\n","price_log_train_std = price_log_train.std()\n","del(price_log_train)\n","\n","y = (price_log - price_log_train_mean) / price_log_train_std\n","y = y.values.reshape(-1, 1)\n","\n","print_elapsed()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"20cb7c48-ab9f-4470-a780-b381e74737ce","_uuid":"c20b4eb8353ec96ebec32cf4023cd5b07a34bf05","collapsed":true,"trusted":false},"outputs":[],"source":["print('Filling null values...')\n","\n","df_train.name.fillna('unk_name', inplace=True)\n","df_train.category_name.fillna('unk_cat', inplace=True)\n","df_train.brand_name.fillna('unk_brand', inplace=True)\n","df_train.item_description.fillna('unk_descr', inplace=True)\n","\n","print_elapsed()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e79191a7-ae28-4be9-afc5-8fc9c2cd73df","_uuid":"206ed9fce06b850a134cf7d898d5ef4116d247fb","collapsed":true,"trusted":false},"outputs":[],"source":["print('Gessing null Brands from name and category...')\n","\n","def concat_categories(x):\n","    return set(x.values)\n","\n","#Getting categories for brands\n","brand_names_categories = dict(df_train[df_train['brand_name'] != 'unk_brand'][['brand_name','category_name']].astype('str').groupby('brand_name').agg(concat_categories).reset_index().values.tolist())\n","\n","#Brands sorted by length (decreasinly), so that longer brand names have precedence in the null brand search\n","brands_sorted_by_size = list(sorted(filter(lambda y: len(y) >= 3, list(brand_names_categories.keys())), key = lambda x: -len(x)))\n","\n","brand_name_null_count = len(df_train.loc[df_train['brand_name'] == 'unk_brand'])\n","\n","#Try to guess the Brand based on Name and Category\n","def brandfinder(name, category):    \n","    for brand in brands_sorted_by_size:\n","        if brand in name and category in brand_names_categories[brand]:\n","            return brand\n","        \n","    return 'unk_brand'\n","\n","train_names_unknown_brands = df_train[df_train['brand_name'] == 'unk_brand'][['name','category_name']].astype('str').values\n","train_estimated_brands = Parallel(n_jobs=4)(delayed(brandfinder)(name, category) for name, category in train_names_unknown_brands)\n","df_train.ix[df_train['brand_name'] == 'unk_brand', 'brand_name'] = train_estimated_brands\n","\n","found = brand_name_null_count-len(df_train.loc[df_train['brand_name'] == 'unk_brand'])\n","print(\"Null brands found: %d from %d\" % (found, brand_name_null_count))\n","\n","print_elapsed()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"055ac420-bcb7-41eb-802e-1f6cb557cc72","_uuid":"549ed16014ce34e18733f2d3097e790feae1daf1","collapsed":true,"trusted":false},"outputs":[],"source":["print('Generating features from category statistics for price ...')\n","\n","CAT_STATS_MIN_COUNT = 5\n","STD_SIGMAS = 2\n","\n","df_train['price_log'] = price_log\n","cats_stats_df = df_train.iloc[train_idxs].groupby(['category_name', 'brand_name', 'shipping']).agg({'category_name': len,\n","                                                     'price_log': [np.median, np.mean, np.std]})\n","cats_stats_df.columns = ['price_log_median', 'price_log_mean', 'price_log_std','count']\n","#Removing categories without a minimum threshold of samples, to avoid price data leak \n","cats_stats_df.drop(cats_stats_df[cats_stats_df['count'] < CAT_STATS_MIN_COUNT].index, inplace=True)\n","cats_stats_df['price_log_std'] = cats_stats_df['price_log_std'].fillna(0)\n","cats_stats_df['price_log_conf_variance'] = cats_stats_df['price_log_std'] / cats_stats_df['price_log_mean']\n","cats_stats_df['count_log'] = np.log1p(cats_stats_df['count'])\n","cats_stats_df['min_expected_log_price'] = (cats_stats_df['price_log_mean'] - cats_stats_df['price_log_std']*STD_SIGMAS).clip(lower=1.0)\n","cats_stats_df['max_expected_log_price'] = (cats_stats_df['price_log_mean'] + cats_stats_df['price_log_std']*STD_SIGMAS)\n","del(df_train['price_log'])\n","\n","len(cats_stats_df)\n","\n","\n","def merge_with_cat_stats(df):\n","    return df.merge(cats_stats_df.reset_index(), how='left', \n","            on=['category_name', 'brand_name', 'shipping'])[['price_log_median', 'price_log_mean', 'price_log_std', \n","                                               'price_log_conf_variance', 'count_log', 'min_expected_log_price', 'max_expected_log_price']].fillna(0).values\n","\n","train_cats_stats_features = merge_with_cat_stats(df_train)\n","\n","cats_stats_features_scaler = StandardScaler(copy=True)\n","X_cats_stats_features_scaled = cats_stats_features_scaler.fit_transform(train_cats_stats_features)\n","\n","del(train_cats_stats_features)\n","\n","print_elapsed()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5becd7c2-468b-4803-a89f-7b34c5ec8752","_uuid":"2c9855d82b693d5b7225daa42de19daada535f40","collapsed":true,"trusted":false},"outputs":[],"source":["gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4fa1275e-20ef-4a9f-9e90-1cd368334137","_uuid":"59a76c8ed78f9c18adb64072191b6e4ce8a9da55","collapsed":true,"trusted":false},"outputs":[],"source":["#Joining the dense features\n","X_float_features = np.hstack([X_item_descr_counts, X_cats_stats_features_scaled])"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f2924ecb-9a5a-4b07-be20-9fb2ee992170","_uuid":"8b0af1bd1dc9de96eaacda0a192c7f6a9ae4010b","collapsed":true,"trusted":false},"outputs":[],"source":["#For Glove, spliting composite words separated by \"-\", because they are rare on Glove\n","regex_tokenizer = RegexpTokenizer(r'[a-z][\\w&]*|[\\d]+[\\.]*[\\w]*|[/!?*:%$\"\\'\\-\\+=\\.,](?![/!?*:%$\"\\'-\\+=\\.,])')\n","\n","def regex_tokenizer_nltk(text):\n","    return regex_tokenizer.tokenize(text.lower())"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7e437bea-fd2c-4391-adff-313b6412da60","_uuid":"645383de8a1151e783972e57cf602175a4a5cb56","collapsed":true,"trusted":false},"outputs":[],"source":["class Tokenizer:\n","    def __init__(self, min_df=10, limit_length_transform=None, tokenizer=str.split, vocabulary=None, \n","                 unk_token_if_ootv=False, workers=1):\n","        self.min_df = min_df\n","        self.tokenizer = tokenizer\n","        self.limit_length_transform = limit_length_transform\n","        self.workers = workers\n","        self.vocab_idx = vocabulary\n","        self.unk_token_if_ootv = unk_token_if_ootv\n","        self.max_len = None  \n","        \n","    def tokenize(self, texts):\n","        #Multi-processing\n","        if self.workers>1:\n","            tokenized_texts = Parallel(n_jobs=self.workers)(delayed(self.tokenizer)(t) for t in texts)\n","        else:\n","            tokenized_texts = [self.tokenizer(t) for t in texts] \n","        return tokenized_texts\n","                \n","    def fit(self, texts):\n","        doc_freq = Counter()\n","\n","        max_len = 0\n","\n","        if type(texts) is list:\n","            tokenized_texts = texts\n","        else: #str\n","                        \n","            tokenized_texts = self.tokenize(texts)\n","            \n","            for sentence in tokenized_texts:\n","                if self.vocab_idx == None:\n","                    doc_freq.update(set(sentence))\n","                max_len = max(max_len, len(sentence))\n","            \n","        self.max_len = max_len\n","\n","        #If the vocabulary is not passed, build from text\n","        if self.vocab_idx == None:\n","            vocab = sorted([t for (t, c) in doc_freq.items() if c >= self.min_df])\n","            self.vocab_idx = {t: (i + 1) for (i, t) in enumerate(vocab)}     \n","\n","\n","    def fit_transform(self, texts):\n","        self.fit(texts)\n","        return self.transform(texts)\n","\n","\n","    def text_to_idx(self, tokenized):\n","        if self.unk_token_if_ootv:            \n","            return [self.vocab_idx[t] if t in self.vocab_idx else self.vocab_idx[UNK_TOKEN] for t in tokenized]\n","        else:\n","            return [self.vocab_idx[t] for t in tokenized if t in self.vocab_idx]\n","    \n","    def transform(self, texts):\n","        n = len(texts)\n","        max_length = self.limit_length_transform or self.max_len\n","        #Value 0 is reserved for the padding character (<PAD>)\n","        result = np.zeros(shape=(n, max_length), dtype=np.int32)\n","        \n","        if self.workers>1:\n","            tokenized_texts = Parallel(n_jobs=self.workers)(delayed(self.tokenizer)(t) for t in texts)\n","        else:\n","            tokenized_texts = [self.tokenizer(t) for t in texts]              \n","        \n","        for i, sentence in enumerate(tokenized_texts):\n","            text = self.text_to_idx(sentence[:max_length])\n","            result[i, :len(text)] = text\n","\n","        return result\n","    \n","    def vocabulary_size(self):\n","        return len(self.vocab_idx) + 1"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c3a17fb3-1f9e-4da8-995e-4eb8edde6835","_uuid":"6526c188d9be07c9cb710dd781dc4384a17c5197","collapsed":true,"trusted":false},"outputs":[],"source":["print('Generating cumulative sub-categories features...')\n","\n","def paths(tokens):\n","    all_paths = ['/'.join(tokens[0:(i+1)]) for i in range(len(tokens))]\n","    return ' '.join(all_paths)\n","\n","whitespace_regex = re.compile(r'\\s+')\n","@lru_cache(1024)\n","def cat_process(cat):\n","    cat = cat.lower()\n","    cat = whitespace_regex.sub('', cat)\n","    split = cat.split('/')\n","    return paths(split)\n","\n","df_train['category_name_cum'] = df_train.category_name.apply(cat_process)\n","\n","cat_tok = Tokenizer(min_df=50)\n","cat_tok.fit(df_train.iloc[np.hstack([train_idxs, train_zeroed_prices_idxs])]['category_name_cum'])\n","X_cat = cat_tok.transform(df_train['category_name_cum'])\n","cat_voc_size = cat_tok.vocabulary_size()\n","\n","print_elapsed()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5f9b5b00-0f64-48eb-a0a5-2e749c6451ab","_uuid":"e783b17deeb3aab751178ffae6817feae92cf96d","collapsed":true,"trusted":false},"outputs":[],"source":["print('Processing vocabulary for name and description....')\n","\n","general_tokenizer = Tokenizer(min_df=30, tokenizer=regex_tokenizer_nltk, workers=4)\n","\n","general_tokenizer.fit(df_train.iloc[np.hstack([train_idxs, train_zeroed_prices_idxs])]['name'] + \" \" \\\n","                    + df_train.iloc[np.hstack([train_idxs, train_zeroed_prices_idxs])]['item_description'])\n","print(\"Text vocabulary size: {}\".format(len(general_tokenizer.vocab_idx)))\n","print_elapsed()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"41578dcf-b869-4ed0-bc98-25fb9975de06","_uuid":"3c22ebeea53e58605b971835939e9f21a54b4dad","collapsed":true,"trusted":false},"outputs":[],"source":["print('Creating vocabulary and loading/generating word embeddings...')\n","\n","words_vocab_with_embeddings = set(general_tokenizer.vocab_idx.keys()).intersection(set(word_embedding_vocab.keys()))\n","words_vocab_no_embeddings = set(general_tokenizer.vocab_idx.keys()) - words_vocab_with_embeddings\n","print('Found word embeddings for corpus: {} from {}'.format(len(words_vocab_with_embeddings), len(general_tokenizer.vocab_idx)))\n","\n","UNK_TOKEN = '<UNK>'\n","PAD_TOKEN = '<PAD>'\n","\n","#Adding words without embedding in the start of the vocabulary\n","words_vocab_general = {t: i for (i, t) in enumerate([PAD_TOKEN, UNK_TOKEN] + sorted(words_vocab_no_embeddings))}\n","words_without_embeddings_vocab_size = len(words_vocab_general)\n","words_with_embeddings_vocab_size = len(words_vocab_with_embeddings)\n","\n","#Adding words with embedding in the end of the vocabulary\n","for (i, t) in enumerate(sorted(words_vocab_with_embeddings)):\n","    words_vocab_general[t] = i+ words_without_embeddings_vocab_size\n","    \n","#Creating inverted vocabulary index\n","custom_inv_vocab_words = dict([(idx,w) for w, idx in list(words_vocab_general.items())])\n","    \n","total_vocab_size = len(words_vocab_general)\n","print('Words without embedding: %d\\tTotal vocabulary size: %d' % (words_without_embeddings_vocab_size, total_vocab_size))\n","    \n","embedding_size = word_embeddings_matrix.shape[1]  \n","print(\"Embedding size: %d\" % (embedding_size))\n","\n","np.random.seed(10)\n","max_abs_embedding_random_value = np.sqrt(2 / embedding_size)\n","glove_scaling_factor = word_embeddings_matrix.max() / max_abs_embedding_random_value\n","\n","#For words available in this GloVE dataset, loading embeddings\n","words_with_embeddings_matrix = word_embeddings_matrix[[word_embedding_vocab[custom_inv_vocab_words[i]] \n","                                                       for i in range(words_without_embeddings_vocab_size, total_vocab_size)]] \\\n","                                / glove_scaling_factor\n","\n","#For words NOT available in this GloVE dataset, generating random embeddings (according to GloVE distribution)\n","words_without_embeddings_matrix = np.random.normal(loc=words_with_embeddings_matrix.mean(), \n","                                                   scale=words_with_embeddings_matrix.std(), \n","                                                   size=(words_without_embeddings_vocab_size, embedding_size))\n","\n","print(\"words_without_embeddings_matrix:\", words_without_embeddings_matrix.shape) \n","print(\"words_without_embeddings_matrix:\", words_with_embeddings_matrix.shape) \n","\n","print_elapsed()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2b17b3e9-477c-437f-9a2f-602fca38508f","_uuid":"1c97cae266c12b9a1244bc7ac4b40c2d590639e2","collapsed":true,"trusted":false},"outputs":[],"source":["#Releasing original GloVE embeddings\n","del(word_embeddings_matrix)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"792c1602-99af-4956-b132-bbb48b8c6b08","_uuid":"506c95b174a756098997c3a3250358bd1a28ae17","collapsed":true,"trusted":false},"outputs":[],"source":["#Maximum number of words of Name and Item_Description to be processed\n","NAME_MAX_LEN = 20\n","ITEM_DESCR_MAX_LEN = 70"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b37cb4dd-179b-4b2b-be0f-2e2a642c77f8","_uuid":"ef84e38cff6379cbddcab6db3ced465f494bddd0","collapsed":true,"trusted":false},"outputs":[],"source":["print('Processing Title...')\n","\n","name_tok = Tokenizer(min_df=0, limit_length_transform=NAME_MAX_LEN, tokenizer=regex_tokenizer_nltk, \n","                     vocabulary=words_vocab_general, unk_token_if_ootv=True, workers=4)\n","name_tok.fit(df_train.iloc[np.hstack([train_idxs, train_zeroed_prices_idxs])]['name'])\n","X_name = name_tok.transform(df_train.name)\n","print(X_name.shape)\n","\n","print_elapsed()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d488d5b4-c9a7-4d4e-adbf-e8898ebb78e2","_uuid":"0ead5d42e360c222793e78baa1adcddb9b0a4321","collapsed":true,"trusted":false},"outputs":[],"source":["print('Processing Description...')\n","\n","desc_tok = Tokenizer(min_df=0, limit_length_transform=ITEM_DESCR_MAX_LEN, tokenizer=regex_tokenizer_nltk, \n","                     vocabulary=words_vocab_general, unk_token_if_ootv=True, workers=4)\n","\n","desc_tok.fit(df_train.iloc[np.hstack([train_idxs, train_zeroed_prices_idxs])].item_description)\n","X_desc = desc_tok.transform(df_train.item_description)\n","print(X_desc.shape)\n","\n","print_elapsed()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4b23492a-c8d8-4938-b612-1f69d7b06a24","_uuid":"2e3c7eee57252e3349c082b16a29860c0d9524bc","collapsed":true,"trusted":false},"outputs":[],"source":["print('Processing Brands...')\n","\n","df_train.brand_name = df_train.brand_name.str.lower()\n","df_train.brand_name = df_train.brand_name.str.replace(' ', '_')\n","\n","brand_cnt = Counter(df_train.brand_name[df_train.brand_name != 'unk_brand'])\n","brands = sorted(b for (b, c) in brand_cnt.items() if c >= 20)\n","brands_idx = {b: (i + 1) for (i, b) in enumerate(brands)}\n","\n","X_brand = df_train.brand_name.apply(lambda b: brands_idx.get(b, 0))\n","X_brand = X_brand.values.reshape(-1, 1) \n","brand_voc_size = len(brands) + 1\n","print(\"Brands vocab. size: {}\".format(brand_voc_size))\n","\n","print_elapsed()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"99dde078-5ad6-474d-8e0a-f5b9fbd0818b","_uuid":"9c51f8f7b2af60f10d6e3631f7dd9f19abcceeef","collapsed":true,"trusted":false},"outputs":[],"source":["print('Processing Item condition and Shipping...')\n","X_item_cond = (df_train.item_condition_id - 1).astype('uint8').values.reshape(-1, 1)\n","X_shipping = df_train.shipping.astype('float32').values.reshape(-1, 1)\n","print_elapsed()"]},{"cell_type":"markdown","metadata":{"_cell_guid":"11149f56-1afd-44c5-a055-61a5e34efb55","_uuid":"1a5d8f5107988c80901187a3c678d8ed4b70be2a"},"source":["### CNN training"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a1457ae4-530d-4175-b445-a8e445c69e07","_uuid":"b248c53fe4ac4dd1b5b82182e8c937f8164aafe0","collapsed":true,"trusted":false},"outputs":[],"source":["def prepare_batches(seq, step):\n","    n = len(seq)\n","    res = []\n","    for i in range(0, n, step):\n","        res.append(seq[i:i+step])\n","    return res"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"61d0a4d8-95b8-4f27-b25f-d3ab2d66a0df","_uuid":"684b86a2fa5689906d38d57d65fc74dd4cdfb40a","collapsed":true,"trusted":false},"outputs":[],"source":["def train_model(session, epochs=4, batch_size=500, eval_each_epoch=True, dropout_input_words=0.0):\n","    print('\\ntraining the model...')\n","    \n","    print_elapsed()\n","    \n","    training_size = train_idxs.shape[0]\n","\n","    for i in range(int(np.ceil(epochs))):\n","        print(\"-----------------EPOCH: {}-------------------\".format(i))\n","        t0 = time()\n","        np.random.seed(i)\n","\n","        epoch_size = training_size\n","        #If the epoch is not int (eg. 2.5), reduces the last epoch size (number of steps)\n","        if i+1 - epochs > 0:\n","            epoch_size = int(training_size*(epochs%1))\n","            \n","        #Training dataset shuffling\n","        batches = prepare_batches(np.random.permutation(train_idxs)[:epoch_size], batch_size)\n","        \n","        for idx in batches:\n","            name_batch = X_name[idx]\n","            desc_batch = X_desc[idx]\n","            \n","            #If set, apply dropout to the input words (kind of data augmentation)\n","            if dropout_input_words > 0:            \n","                name_mask = (np.random.uniform(0,1, size=name_batch.shape) >= dropout_input_words).astype(np.int32)\n","                name_batch = name_batch * name_mask\n","\n","                desc_mask = (np.random.uniform(0,1, size=desc_batch.shape) >= dropout_input_words).astype(np.int32)\n","                desc_batch = desc_batch * desc_mask\n","        \n","            feed_dict = {\n","                place_name: name_batch,\n","                place_desc: desc_batch,\n","                place_brand: X_brand[idx],\n","                place_cat: X_cat[idx],\n","                place_cond: X_item_cond[idx],\n","                place_ship: X_shipping[idx],\n","                place_float_stats: X_float_features[idx],\n","                place_y: y[idx],\n","                place_training: True\n","            }\n","            session.run(train_step, feed_dict=feed_dict)\n","\n","        took = time() - t0\n","        print('epoch %d took %.3fs' % (i, took))\n","        \n","        if eval_each_epoch and i < epochs-1:\n","            train_set_evaluation(sess)\n","            print_elapsed()\n","            \n","            eval_set_evaluation(session)\n","            \n","        print_elapsed()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f25927e9-52a4-46c8-8c7b-04510715b17b","_uuid":"61322cee3fc1a6671a4f5dbd2950f9a9decb804d","collapsed":true,"trusted":false},"outputs":[],"source":["def eval_set_evaluation(session, save_results_to=None):\n","    print('\\nEVAL SET Evaluation')\n","\n","    y_pred_norm_eval = np.zeros(valid_size)\n","\n","    EVAL_BATCH_SIZE = 5000\n","\n","    batches = prepare_batches(valid_idxs, EVAL_BATCH_SIZE)\n","\n","    for b, idx in enumerate(batches):\n","        feed_dict = {\n","            place_name: X_name[idx],\n","            place_desc: X_desc[idx],\n","            place_brand: X_brand[idx],\n","            place_cat: X_cat[idx],\n","            place_cond: X_item_cond[idx],\n","            place_ship: X_shipping[idx],\n","            place_float_stats: X_float_features[idx],\n","            place_training: False\n","        }\n","        batch_pred = session.run(out, feed_dict=feed_dict)\n","        start_idx = (b*EVAL_BATCH_SIZE)\n","        y_pred_norm_eval[start_idx:min(start_idx+EVAL_BATCH_SIZE, valid_size)] = batch_pred[:, 0]\n","        \n","    y_adjusted_pred_eval = np.expm1(y_pred_norm_eval * price_log_train_std + price_log_train_mean)\n","\n","    print(\"Eval set RMSLE: {}\".format(rmsle(price[valid_idxs].values, y_adjusted_pred_eval)))    \n","\n","    if not SUBMISSION and save_results_to != None:\n","        print('Saving CNN eval results to \"valid_cnn_predictions.csv\"...')    \n","        df_out = pd.DataFrame()\n","        df_out['train_id'] = df_train.iloc[valid_idxs]['train_id']\n","        df_out['price'] = price[valid_idxs].values\n","        df_out['pred_price'] = y_adjusted_pred_eval\n","        df_out.to_csv(save_results_to, index=False)    \n","\n","    return y_pred_norm_eval"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"53c72ca7-baa5-4859-a1a7-1d0d23b1efce","_uuid":"380b55dd20b75831b6e2bf123fe73859cbd9416a","collapsed":true,"scrolled":true,"trusted":false},"outputs":[],"source":["def train_set_evaluation(session):\n","    print('\\nTRAIN SET Evaluation')\n","\n","    y_pred_norm = np.zeros(train_size)\n","\n","    EVAL_BATCH_SIZE = 5000\n","\n","    batches = prepare_batches(train_idxs, EVAL_BATCH_SIZE)\n","\n","    for b, idx in enumerate(batches):\n","        feed_dict = {\n","            place_name: X_name[idx],\n","            place_desc: X_desc[idx],\n","            place_brand: X_brand[idx],\n","            place_cat: X_cat[idx],\n","            place_cond: X_item_cond[idx],\n","            place_ship: X_shipping[idx],\n","            place_float_stats: X_float_features[idx],\n","            place_training: False\n","        }\n","        batch_pred = session.run(out, feed_dict=feed_dict)\n","        start_idx = (b*EVAL_BATCH_SIZE)\n","        \n","        y_pred_norm[start_idx:min(start_idx+EVAL_BATCH_SIZE, train_size)] = batch_pred[:, 0]                \n","\n","    y_pred_train = y_pred_norm * price_log_train_std + price_log_train_mean\n","    y_adjusted_pred_train = np.expm1(y_pred_train)\n","\n","    print(\"Train set RMSLE: {}\".format(rmsle(price[train_idxs].values, y_adjusted_pred_train)))\n","    \n","    return price[train_idxs].values, y_adjusted_pred_train"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7477ed9d-5421-4536-973a-1cb5f15360bc","_uuid":"fa7b412da7ced5e63261f40e6b7b20d73dea1e98","collapsed":true,"trusted":false},"outputs":[],"source":["def save_test_set_predictions(preds, filename):\n","    df_out = pd.DataFrame()\n","    df_out['test_id'] = range(0,preds.shape[0])\n","    df_out['price'] = preds\n","\n","    df_out.to_csv(filename, index=False)    \n","    \n","    print_elapsed('Predictions exported to {}'.format(filename))"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"735280f3-cfe0-45c5-9ac8-a94faf0c8316","_uuid":"69695a2f1da065fb7c03a86ccb5b723ac5f9cba1","collapsed":true,"trusted":false},"outputs":[],"source":["def generate_test_submission(session):\n","    print('Reading and generating features for test data...')\n","\n","    #TODO: Read data (read_table) in batches (ex: read smaller batches using chunksize) \n","    #to better support larger testset in 2nd phase\n","    df_test = pd.read_csv('../input/mercari-price-suggestion-challenge/test_stg2.tsv', sep='\\t')\n","    \n","    #Filling nulls\n","    df_test.name.fillna('unk_name', inplace=True)\n","    df_test.category_name.fillna('unk_cat', inplace=True)\n","    df_test.brand_name.fillna('unk_brand', inplace=True)\n","    df_test.item_description.fillna('unk_brand', inplace=True)\n","    \n","    #Guessing null Brands\n","    test_names_unknown_brands = df_test[df_test['brand_name'] == 'unk_brand'][['name','category_name']].astype('str').values\n","    test_estimated_brands = Parallel(n_jobs=7)(delayed(brandfinder)(name, category) for name, category in test_names_unknown_brands)\n","    df_test.ix[df_test['brand_name'] == 'unk_brand', 'brand_name'] = test_estimated_brands\n","\n","    #Processing categories\n","    df_test.category_name = df_test.category_name.apply(cat_process)\n","    df_test.brand_name = df_test.brand_name.str.lower()\n","    df_test.brand_name = df_test.brand_name.str.replace(' ', '_')\n","    \n","    #Generating statistic features for Description \n","    X_item_descr_counts_test = item_descr_counts_scaler.transform(np.vstack(df_test['item_description'].astype(str) \\\n","                                                                            .apply(extract_counts).values))\n","\n","    #Generating features of price statistics by categories\n","    X_cats_stats_features_scaled_test = cats_stats_features_scaler.transform(merge_with_cat_stats(df_test))\n","    \n","    #Joining dense features\n","    X_float_features_test = np.hstack([X_item_descr_counts_test, X_cats_stats_features_scaled_test])\n","    \n","    #Tokenizing category name\n","    X_cat_test = cat_tok.transform(df_test.category_name)\n","    X_name_test = name_tok.transform(df_test.name)\n","\n","    #Tokenizing description\n","    X_desc_test = desc_tok.transform(df_test.item_description)\n","\n","    #Tokenizing category name\n","    X_item_cond_test = (df_test.item_condition_id - 1).astype('uint8').values.reshape(-1, 1)\n","    X_shipping_test = df_test.shipping.astype('float32').values.reshape(-1, 1)\n","\n","    #Processing brands\n","    X_brand_test = df_test.brand_name.apply(lambda b: brands_idx.get(b, 0))\n","    X_brand_test = X_brand_test.values.reshape(-1, 1)\n","    \n","    print_elapsed('Finish generating features for test set')\n","    \n","    \n","    print('Prediction for test set using the trained CNN model...')\n","\n","    n_test = len(df_test)\n","    y_pred = np.zeros(n_test)\n","\n","    test_idx = np.arange(n_test)\n","    batches = prepare_batches(test_idx, 5000)\n","\n","    for idx in batches:\n","        feed_dict = {\n","            place_name: X_name_test[idx],\n","            place_desc: X_desc_test[idx],\n","            place_brand: X_brand_test[idx],\n","            place_cat: X_cat_test[idx],\n","            place_cond: X_item_cond_test[idx],\n","            place_ship: X_shipping_test[idx],            \n","            place_float_stats: X_float_features_test[idx],\n","            place_training: False\n","        }\n","        \n","        batch_pred = session.run(out, feed_dict=feed_dict)\n","        y_pred[idx] = batch_pred[:, 0]\n","\n","    print_elapsed()\n","    \n","    return y_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4dea2044-e977-47ea-97a5-f9fdaa69ba5b","_uuid":"f2ae903aafbbb753830c1dc05d66e51dd71a2ec9","collapsed":true,"trusted":false},"outputs":[],"source":["gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"27965dff-c8d2-4967-9d5d-f2e604e68632","_uuid":"c52acb4ee807b898f523b2d796d67aeeb3c779ff","collapsed":true,"scrolled":true,"trusted":false},"outputs":[],"source":["print('Defining the model...')\n","\n","def conv1d(inputs, num_filters, filter_size, pool_size, is_training, reg=0.0, activation=None, padding='same'):\n","\n","    out = tf.layers.conv1d(\n","        inputs=inputs, filters=num_filters, padding=padding,\n","        kernel_size=filter_size,\n","        strides=1,\n","        activation=activation,      \n","        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n","        kernel_regularizer=tf.contrib.layers.l2_regularizer(reg),\n","        )\n","    \n","    out = tf.layers.max_pooling1d(out, pool_size=pool_size, strides=1, padding='valid')\n","                                       \n","    return out\n","\n","def dense(X, size, reg=0.0, activation=None):\n","    out = tf.layers.dense(X, units=size, activation=activation, \n","                     kernel_initializer=tf.contrib.layers.xavier_initializer(),\n","                     kernel_regularizer=tf.contrib.layers.l2_regularizer(reg))\n","    return out\n","\n","def embed(inputs, size, dim):\n","    std = np.sqrt(2 / dim)\n","    emb = tf.Variable(tf.random_uniform([size, dim], -std, std))\n","    lookup = tf.nn.embedding_lookup(emb, inputs)\n","    return lookup\n","\n","\n","dnn_settings = {\n","    #Training params\n","    'lr_initial': 0.002, \n","    'lr_decay_rate': 0.94, \n","    'lr_num_epochs_per_decay': 0.1,\n","    'batch_size': 256,\n","    'epochs': 3.0, #2.5, #3\n","    \n","    'dropout_input_words': 0.05, #Randomly set to zero (<PAD>) some words of the input text (data augmentation)\n","    'dropout_rate': 0.00,\n","    'l2_reg': 0.0, \n","    'main_batch_norm_decay': 0.93,\n","\n","     #Model params\n","    'cnn_filter_sizes': [3], \n","\n","    'name_seq_len': NAME_MAX_LEN,\n","    'name_num_filters': 128,\n","    'name_avg_embedding_num_words': 5,    \n","\n","    'desc_seq_len': ITEM_DESCR_MAX_LEN,\n","    'desc_num_filters': 96, \n","    'descr_avg_embedding_num_words': 20,\n","\n","    'brand_embeddings_dim': 32, \n","\n","    'cat_embeddings_dim': 32, \n","    'cat_seq_len': X_cat.shape[1],\n","\n","    'word_embeddings_size': embedding_size,\n","    'word_embeddings_max_norm': 0.45,\n","}\n","\n","print(\"SETTINGS:\", dnn_settings)\n","\n","\n","print()\n","\n","\n","graph = tf.Graph()\n","graph.seed = 1\n","\n","ALLOW_SOFT_PLACEMENT=True\n","LOG_DEVICE_PLACEMENT=False\n","\n","with graph.as_default():\n","    \n","    #As GPUs were not available on Kaggle Kernelm using them only for local development \n","    with tf.device('/cpu:0' if SUBMISSION else '/gpu:0'):    \n","    \n","        session_conf = tf.ConfigProto(\n","          allow_soft_placement=ALLOW_SOFT_PLACEMENT,\n","          log_device_placement=LOG_DEVICE_PLACEMENT,\n","          intra_op_parallelism_threads = 4,\n","          inter_op_parallelism_threads = 4)\n","\n","        sess = tf.Session(config=session_conf)\n","        with sess.as_default():\n","            \n","            with tf.name_scope(\"embedding\"):          \n","                \n","                #Loading GloVE word embeddings for available words \n","                #Reference: https://ireneli.eu/2017/01/17/tensorflow-07-word-embeddings-2-loading-pre-trained-vectors/\n","                words_with_embedding_placeholder = tf.placeholder(tf.float32, [words_with_embeddings_vocab_size,\n","                                                                               dnn_settings['word_embeddings_size']])\n","                words_with_embedding_variable = tf.Variable(tf.constant(0.0, shape=[words_with_embeddings_vocab_size,\n","                                                                                    dnn_settings['word_embeddings_size']]),\n","                    #Best results were obtaining using Glove to initialize embeddings, \n","                    #and letting them to be trained for the prediction task\n","                    trainable=True,                \n","                    name=\"words_with_embedding\")\n","                words_with_embedding_init = words_with_embedding_variable.assign(words_with_embedding_placeholder)\n","                \n","                #Loading random word embeddings for OOTV words\n","                words_without_embedding_placeholder = tf.placeholder(tf.float32, [words_without_embeddings_vocab_size,\n","                                                                                  dnn_settings['word_embeddings_size']])\n","                words_without_embedding_variable = tf.Variable(tf.constant(0.0, shape=[words_without_embeddings_vocab_size,\n","                                                                                       dnn_settings['word_embeddings_size']]),\n","                    trainable=True,                \n","                    name=\"words_without_embedding\")\n","                words_without_embedding_init = words_without_embedding_variable.assign(words_without_embedding_placeholder)\n","                \n","                #Creating a regularizer for embeddings values\n","                word_embedding_regularizer = tf.nn.l2_loss(words_without_embedding_variable)\n","\n","                #Concatenating GloVE embeddings and random embeddings in a single variable\n","                words_embedding_variable = tf.concat([words_without_embedding_variable, words_with_embedding_variable], \n","                                                    axis=0)\n","\n","            \n","            #Model input features\n","            place_name = tf.placeholder(tf.int32, shape=(None, dnn_settings['name_seq_len']))\n","            place_desc = tf.placeholder(tf.int32, shape=(None, dnn_settings['desc_seq_len']))\n","            place_brand = tf.placeholder(tf.int32, shape=(None, 1))\n","            place_cat = tf.placeholder(tf.int32, shape=(None, dnn_settings['cat_seq_len']))\n","            place_ship = tf.placeholder(tf.float32, shape=(None, 1))\n","            place_cond = tf.placeholder(tf.uint8, shape=(None, 1))\n","            place_float_stats = tf.placeholder(dtype=tf.float32, shape=(None, X_float_features.shape[1]))\n","            \n","            #Output feature\n","            place_y = tf.placeholder(dtype=tf.float32, shape=(None, 1))\n","            \n","            #Flag to indicate whether the graph is being trained or used for inference\n","            place_training = tf.placeholder(tf.bool, shape=(), )\n","            \n","            #Creating embedding layer for categorical features Brands and Categories\n","            brand = embed(place_brand, brand_voc_size, dnn_settings['brand_embeddings_dim'])\n","            cat = embed(place_cat, cat_voc_size, dnn_settings['cat_embeddings_dim'])\n","                        \n","            #Looking up embeddings for each word            \n","            name = tf.nn.embedding_lookup(words_embedding_variable, place_name, max_norm=dnn_settings['word_embeddings_max_norm'])\n","            desc = tf.nn.embedding_lookup(words_embedding_variable, place_desc, max_norm=dnn_settings['word_embeddings_max_norm'])\n","            print(\"name.shape\", name.shape)\n","            print(\"desc.shape\", desc.shape)\n","            \n","            #Creating a special layer to average embeddings of the first words of the name and description\n","            #under the assumption that they are the most representative ones\n","            name_mean_embeddings = tf.reduce_mean(name[:,:dnn_settings['name_avg_embedding_num_words'],:], axis=1)\n","            print(\"name_mean_embeddings.shape\", name_mean_embeddings.shape)\n","            desc_mean_embeddings = tf.reduce_mean(desc[:,:dnn_settings['descr_avg_embedding_num_words'],:], axis=1)\n","            print(\"desc_mean_embeddings.shape\", desc_mean_embeddings.shape)\n","            \n","            \n","            conv_layers = []\n","            for filter_size in dnn_settings['cnn_filter_sizes']:\n","                with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n","                    name_conv = conv1d(name, num_filters=dnn_settings['name_num_filters'], filter_size=filter_size, \n","                                       pool_size=dnn_settings['name_seq_len'], \n","                                       is_training=place_training, reg=dnn_settings['l2_reg'], activation=tf.nn.relu)                \n","                    name_conv = tf.contrib.layers.flatten(name_conv)\n","                    print((\"conv-maxpool-%s NAME\" % filter_size), name_conv.shape)\n","                    conv_layers.append(name_conv)\n","\n","                    desc_conv = conv1d(desc, num_filters=dnn_settings['desc_num_filters'], filter_size=filter_size, \n","                                       pool_size=dnn_settings['desc_seq_len'], \n","                                       is_training=place_training, reg=dnn_settings['l2_reg'], activation=tf.nn.relu)\n","                    desc_conv = tf.contrib.layers.flatten(desc_conv)\n","                    print((\"conv-maxpool-%s DESCRIPTION\" % filter_size), desc_conv.shape)\n","                    conv_layers.append(desc_conv)\n","\n","            brand = tf.contrib.layers.flatten(brand)\n","            print(brand.shape)\n","\n","            #cat = tf.layers.average_pooling1d(cat, pool_size=dnn_settings['cat_seq_len'], strides=1, padding='valid')\n","            cat = tf.contrib.layers.flatten(cat)\n","            print(cat.shape)\n","\n","            ship = place_ship\n","            print(ship.shape)\n","\n","            cond = tf.one_hot(place_cond, 5)\n","            cond = tf.contrib.layers.flatten(cond)\n","            print(cond.shape)\n","            \n","            float_stats = place_float_stats\n","            float_stats = tf.contrib.layers.flatten(float_stats)\n","            print(float_stats.shape)\n","              \n","            #Joining all layers outputs for a sequence of Fully Connected layers\n","            out = tf.concat(conv_layers + [name_mean_embeddings, desc_mean_embeddings, brand, cat, ship, cond, float_stats], axis=1)\n","            print('concatenated dim:', out.shape)\n","\n","\n","            out = tf.contrib.layers.batch_norm(out, decay=dnn_settings['main_batch_norm_decay'], \n","                                               center=True, scale=False, epsilon=0.001,           \n","                                               is_training=place_training)\n","            \n","\n","            #out = tf.layers.dropout(out, rate=dropout_rate, training=place_training)\n","            out = dense(out, 256, reg=dnn_settings['l2_reg'], activation=tf.nn.relu)\n","\n","            #out = tf.layers.dropout(out, rate=dropout_rate, training=place_training)\n","            out = dense(out, 128, reg=dnn_settings['l2_reg'], activation=tf.nn.relu)\n","            \n","            out = tf.contrib.layers.layer_norm(tf.concat([out, ship, cond, float_stats], axis=1))\n","\n","            out = dense(out, 1, reg=dnn_settings['l2_reg'])\n","\n","            reg_loss = tf.losses.get_regularization_loss() + dnn_settings['l2_reg']*word_embedding_regularizer\n","            \n","            #Computing the loss, with L2 regularization\n","            loss = tf.losses.mean_squared_error(place_y, out) + reg_loss\n","            rmse = tf.sqrt(loss)\n","            \n","            #Setting learning rate decay\n","            lr_decay_steps = int(train_size / dnn_settings['batch_size'] * dnn_settings['lr_num_epochs_per_decay'])\n","            \n","            global_step = tf.Variable(0, trainable=False)\n","            learning_rate_decay = tf.train.exponential_decay(dnn_settings['lr_initial'],\n","                                          global_step,\n","                                          lr_decay_steps,\n","                                          dnn_settings['lr_decay_rate'],\n","                                          staircase=True,\n","                                          name='exponential_decay_learning_rate')\n","            \n","            opt = tf.train.AdamOptimizer(learning_rate=learning_rate_decay,\n","                                         beta1=0.9,\n","                                         beta2=0.999,\n","                                         epsilon=1e-08\n","                                        )\n","            \n","            #Necessary to run update ops for batch_norm\n","            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n","            with tf.control_dependencies(update_ops):                \n","                train_step = opt.minimize(loss=loss, global_step=global_step)\n","\n","            print(\"Initializing variables\")\n","            init = tf.global_variables_initializer()        \n","            sess.run(init)\n","            print_elapsed()\n","                  \n","            #Initializing word embeddings variable\n","            sess.run([words_without_embedding_init,\n","                      words_with_embedding_init], \n","                     feed_dict={words_without_embedding_placeholder: words_without_embeddings_matrix,\n","                                words_with_embedding_placeholder: words_with_embeddings_matrix})\n","\n","            #Training the model\n","            train_model(sess, epochs=dnn_settings['epochs'], batch_size=dnn_settings['batch_size'], \n","                        dropout_input_words=dnn_settings['dropout_input_words'], \n","                        eval_each_epoch=not SUBMISSION)\n","            print_elapsed()\n","\n","            if not SUBMISSION:\n","                #Evaluating train set\n","                train_actual_prices_debug, train_pred_prices_debug = train_set_evaluation(sess)\n","                print_elapsed()\n","\n","                #Evaluating validation set\n","                cnn_pred_eval = eval_set_evaluation(sess)\n","                print_elapsed()\n","\n","            if SUBMISSION:\n","                #Generating output CSV file with the predictions for test set\n","                cnn_pred_test = generate_test_submission(sess)                \n","                cnn_pred_test_scaled = np.expm1(cnn_pred_test * price_log_train_std + price_log_train_mean)\n","                save_test_set_predictions(cnn_pred_test_scaled, 'submission_cnn.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"1718f342-2a34-438a-a574-14d198d1a7b0","_uuid":"7c3c9b3c9154e341deec35292c86fafe19dcfb03","collapsed":true,"trusted":false},"outputs":[],"source":["print_elapsed('Finished script')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}
